{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Pickle Viewer\n",
    "\n",
    "This notebook loads and displays the contents of the benchmarks pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append('.')\n",
    "from src.excel_processor import Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the benchmarks from the pickle file\n",
    "# Use absolute path to ensure the file can be found\n",
    "current_dir = Path.cwd()\n",
    "pickle_path = current_dir / \"data\" / \"processed\" / \"benchmarks.pkl\"\n",
    "\n",
    "print(f\"Looking for pickle file at: {pickle_path}\")\n",
    "\n",
    "if pickle_path.exists():\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        benchmarks = pickle.load(f)\n",
    "    print(f\"Total benchmarks: {len(benchmarks)}\")\n",
    "else:\n",
    "    print(f\"Pickle file not found at {pickle_path}\")\n",
    "    # Try alternative paths\n",
    "    alt_path = Path(\"data/processed/benchmarks.pkl\")\n",
    "    if alt_path.exists():\n",
    "        with open(alt_path, 'rb') as f:\n",
    "            benchmarks = pickle.load(f)\n",
    "        print(f\"Loaded from alternative path. Total benchmarks: {len(benchmarks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View sample benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 benchmarks\n",
    "for i, (key, value) in enumerate(list(benchmarks.items())[:5]):\n",
    "    print(f\"\\n{i+1}. Benchmark ID: {key}\")\n",
    "    print(f\"   Grade Level: {value.grade_level}\")\n",
    "    print(f\"   Subject: {value.subject}\")\n",
    "    print(f\"   Definition: {value.definition[:100]}...\" if len(value.definition) > 100 else f\"   Definition: {value.definition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame for easier viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the benchmarks to a DataFrame\n",
    "data = []\n",
    "for key, value in benchmarks.items():\n",
    "    data.append({\n",
    "        \"ID\": key,\n",
    "        \"Grade Level\": value.grade_level,\n",
    "        \"Subject\": value.subject,\n",
    "        \"Definition\": value.definition\n",
    "    })\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Grade Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique grade levels\n",
    "grade_levels = df[\"Grade Level\"].unique()\n",
    "print(\"Available grade levels:\")\n",
    "print(grade_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by a specific grade level (change as needed)\n",
    "grade_to_filter = \"K\"  # Change this to filter by different grades\n",
    "\n",
    "filtered_df = df[df[\"Grade Level\"] == grade_to_filter]\n",
    "print(f\"Found {len(filtered_df)} benchmarks for grade {grade_to_filter}\")\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by Benchmark ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for benchmarks by ID pattern\n",
    "search_term = \"MA.K\"  # Change this to search for different patterns\n",
    "\n",
    "search_results = df[df[\"ID\"].str.contains(search_term)]\n",
    "print(f\"Found {len(search_results)} benchmarks matching '{search_term}'\")\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View a specific benchmark in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a specific benchmark (change the ID as needed)\n",
    "benchmark_id = list(benchmarks.keys())[0]  # First benchmark by default\n",
    "\n",
    "if benchmark_id in benchmarks:\n",
    "    benchmark = benchmarks[benchmark_id]\n",
    "    print(f\"Benchmark ID: {benchmark_id}\")\n",
    "    print(f\"Grade Level: {benchmark.grade_level}\")\n",
    "    print(f\"Subject: {benchmark.subject}\")\n",
    "    print(f\"\\nDefinition:\")\n",
    "    print(benchmark.definition)\n",
    "else:\n",
    "    print(f\"Benchmark {benchmark_id} not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
